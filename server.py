# server.py  â€”â€”  FastMCP server å…¥å£
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from mcp.server.fastmcp import FastMCP, Context
import sys
from extractor.DrissionPage_Downloader import OUTPUT_DIR, CONFIG, generate_mhtml_filename
from DrissionPage import Chromium, ChromiumOptions  # type: ignore
from dotenv import load_dotenv
import pytesseract
import subprocess


def debug(msg: str):
    # æ‰€æœ‰è°ƒè¯•ä¿¡æ¯éƒ½æ‰“å°åˆ° stderrï¼Œé¿å…å¹²æ‰° stdio JSON-RPC æµ
    print(msg, file=sys.stderr)

debug("== InfoExtractor server starting ==")

# å…¨å±€æµè§ˆå™¨å˜é‡
BROWSER: Chromium | None = None

def get_browser() -> Chromium:
    """
    åˆå§‹åŒ–å¹¶è¿”å›žå…¨å±€ Chromium å®žä¾‹ã€‚åŽç»­è°ƒç”¨éƒ½ä¼šå¤ç”¨åŒä¸€ä¸ªæµè§ˆå™¨ã€‚
    """
    global BROWSER
    if BROWSER is None:
        opts = ChromiumOptions()
        opts.incognito()
        opts.auto_port(True)
        BROWSER = Chromium(addr_or_opts=opts)
        debug(">> Chromium browser launched")
    return BROWSER

def quit_browser():
    """
    å…³é—­å…¨å±€æµè§ˆå™¨å®žä¾‹ã€‚
    """
    global BROWSER
    if BROWSER is not None:
        debug(">> Quitting Chromium browser")
        BROWSER.quit()
        BROWSER = None

@asynccontextmanager
async def lifespan(app: FastMCP) -> AsyncIterator[dict]:
    # å¯åŠ¨æ—¶åˆå§‹åŒ–æ‰€æœ‰ä¾èµ–çŽ¯å¢ƒ
    debug(">> lifespan: initializing resources")
    # å¦‚æžœä½ å¸Œæœ›ä¸€å¯åŠ¨å°±åˆå§‹åŒ–æµè§ˆå™¨ï¼Œå¯ä»¥åœ¨è¿™é‡Œè°ƒç”¨ get_browser()
    get_browser()
    yield {}
    debug(">> lifespan: cleaning up resources")
    quit_browser()
    debug(">> Chromium browser quit")

mcp = FastMCP(
    "InfoExtractor",
    lifespan=lifespan,
    dependencies=[
        # åœ¨è¿™é‡Œåˆ—ä¾èµ–ï¼Œuv/pip ä¼šè‡ªåŠ¨å®‰è£…
        "drissionpage",
        "beautifulsoup4",
        "pytesseract",
        "python-dotenv",
        "openai",
    ],
)

@mcp.tool()
async def download_urls_tool(
    urls: list[str],
    *,
    ctx: Context | None = None,
) -> dict:
    debug(f"--> download_urls_tool called with: {urls}")
    browser = get_browser()
    saved_paths: dict[str, str] = {}

    for url in urls:
        try:
            debug(f"----> Downloading {url}")
            tab = browser.new_tab()
            tab.get(url)
            tab.wait(CONFIG["waitTime"])

            filename = generate_mhtml_filename(url)
            tab.save(path=str(OUTPUT_DIR), name=filename[:-6], as_pdf=False)
            tab.close()

            full_path = str(OUTPUT_DIR / filename)
            saved_paths[url] = full_path
            debug(f"----> Saved to {full_path}")
        except Exception as e:
            debug(f"!!!! download failed for {url}: {e}")
            saved_paths[url] = f"ERROR: {e}"

    debug(f"<-- download_urls_tool result: {saved_paths}")
    return {"mhtml_files": saved_paths}

@mcp.tool()
async def extract_mhtml_tool(*, ctx: Context | None = None) -> dict:
    debug("--> extract_mhtml_tool called")
    from extractor.pipeline import extract_mhtml

    result = await extract_mhtml(ctx=ctx)

    if ctx:
        await ctx.info(f"âœ… extract_mhtml_tool exit, got: {result}")

    # await the info coroutine
    await ctx.info("ðŸš€ extract_mhtml_tool entry")
    debug(f"<-- extract_mhtml_tool result: {result}")
    return result

@mcp.tool()
async def test(ctx: Context) -> dict:
    await ctx.info("ping")
    return {"ok": True}


if __name__ == "__main__":
    debug("== entering mcp.run() ==")
    mcp.run()
    debug("== mcp.run() has exited ==")
