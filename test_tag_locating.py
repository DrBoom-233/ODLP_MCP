"""
æµ‹è¯•ç¨‹åº - Tag_Locating.py çš„ç‹¬ç«‹æµ‹è¯•ç‰ˆæœ¬
è¿™ä¸ªç¨‹åºç§»é™¤äº†MCPç›¸å…³ä¾èµ–ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨Pythonè¿è¡Œ
"""

from __future__ import annotations

import asyncio
import io
import json
import os
import random
import re
import sys
import time
import argparse
from collections import defaultdict
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from bs4 import BeautifulSoup, Tag
from playwright.async_api import async_playwright, Page

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç¼–ç å…¼å®¹ï¼šè§£å†³ Windows ä¸‹ä¸­æ–‡è¾“å‡ºä¹±ç 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç›®å½•å¸¸é‡ï¼šåŸºäºè„šæœ¬ä½ç½®å®šä½é¡¹ç›®æ ¹å’Œ mhtml è¾“å‡ºç›®å½•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THIS_DIR = Path(__file__).resolve().parent             # é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = THIS_DIR                                # é¡¹ç›®æ ¹ç›®å½•
MHTML_DIR = PROJECT_ROOT / "mhtml_output"              # mhtml_output
EXTRACTOR_DIR = PROJECT_ROOT / "extractor"             # extractor

# ============================================================================
# ğŸ”‘  è¾…åŠ©å·¥å…·
# ============================================================================

def _escape_regex(text: str) -> str:
    """å¯¹æ­£åˆ™å…ƒå­—ç¬¦è½¬ä¹‰"""
    return re.escape(text)


def _similar_ratio(a: str, b: str) -> float:
    """å¤§å°å†™æ— å…³çš„ SequenceMatcher ç›¸ä¼¼åº¦ [0â€‘1]"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def get_dom_path(tag: Tag) -> str:
    """è·å–ä»å½“å‰æ ‡ç­¾åˆ°æ ¹èŠ‚ç‚¹çš„ DOM è·¯å¾„ï¼ˆtagName ä¸²è”ï¼‰ã€‚"""
    segments = []
    while tag is not None:
        segments.append(tag.name)
        tag = tag.parent  # type: ignore[attrâ€‘defined]
    return " > ".join(reversed(segments))


# ----------------------------------------------------------------------------
# æ ¸å¿ƒ âœ¨ get_item_paths âœ¨ï¼šä¸‰æ®µå¼å®šä½ç­–ç•¥å®ç°
# ----------------------------------------------------------------------------

def get_item_paths(soup: BeautifulSoup, product_names: List[str]) -> Dict[str, List[Tag]]:
    """æ ¹æ® *product_names* åœ¨ DOM ä¸­å®šä½æ ‡ç­¾ï¼ˆæˆ–å…¶å…¬å…±çˆ¶å…ƒç´ ï¼‰çš„è·¯å¾„ã€‚

    **å®ç°é€»è¾‘**
    1. å°è¯• *ç²¾ç¡®/åŒ…å«* åŒ¹é…æ•´ä¸ªå­—ç¬¦ä¸²ï¼›æˆåŠŸåˆ™è®°å½•ã€‚
    2. è‹¥å¤±è´¥ â†’ **æ¨¡ç³Š**ï¼šä½¿ç”¨ç›¸ä¼¼åº¦ > 0.65 çš„å…ƒç´ åšç²—å®šä½ï¼ˆç²—å®¹å™¨ï¼‰ã€‚
    3. åœ¨ç²—å®¹å™¨å†…éƒ¨ **æ ‡ç­¾çº§åˆ†è¯**ï¼šæŒ‰å…ƒç´ è¾¹ç•Œåˆ‡è¯ï¼›å¯¹é½åŸå­—ç¬¦ä¸²åˆ‡åˆ† tokenã€‚
    4. å¯¹æ¯ä¸ª token å†åš **é€è¯ç²¾ç¡®å®šä½**ï¼›æ‰¾æœ€ä½å…¬å…±çˆ¶å…ƒç´ ä½œä¸ºæœ€ç»ˆæ ‡ç­¾ã€‚
    5. Fallbackï¼šä»æ— æ³•å®šä½ï¼Œåˆ™æ‹†è¯åç›´æ¥åœ¨æ–‡æ¡£çº§æœç´¢ï¼Œæ¯è¯ç‹¬ç«‹å¤„ç†ã€‚
    """

    paths: Dict[str, List[Tag]] = defaultdict(list)

    # é¢„ç¼–è¯‘å¸¸ç”¨å‡½æ•°
    def exact_or_contains(txt: str) -> Optional[Tag]:
        # å®Œæ•´åŒ¹é…
        exact = soup.find(lambda t: t.string and t.string.strip().lower() == txt.lower())
        if exact:
            return exact
        # å­ä¸²åŒ…å«
        return soup.find(lambda t: t.string and txt.lower() in t.string.lower())

    for raw in product_names:
        raw_clean = raw.strip()
        if not raw_clean:
            continue

        # â€”â€”â€” â‘  ç²¾ç¡® / åŒ…å«åŒ¹é… â€”â€”â€”
        tag = exact_or_contains(raw_clean)
        if tag:
            paths[get_dom_path(tag)].append(tag)
            continue  # âœ… ç›´æ¥æ‰¾åˆ°ï¼Œè·³è¿‡åç»­

        # â€”â€”â€” â‘¡ æ¨¡ç³Šå®šä½ï¼šæ‰¾ç›¸ä¼¼åº¦æœ€é«˜çš„å…ƒç´ ä½œä¸º"ç²—å®¹å™¨" â€”â€”â€”
        # å…ˆç²—æš´æ‹¿æ‰€æœ‰åŒ…å«å•è¯çš„å…ƒç´ ï¼ˆé˜²æ­¢å…¨å±€éå†è€—æ—¶ï¼‰
        word_pat = re.compile(_escape_regex(raw_clean.split()[0]), re.I)
        candidates = [t for t in soup.find_all(string=word_pat) if isinstance(t, str)]
        best_container: Optional[Tag] = None
        best_score = 0.0
        for text_node in candidates:
            parent_el = text_node.parent  # type: ignore[assignment]
            text_val = text_node.strip()
            score = _similar_ratio(text_val, raw_clean)
            if score > best_score:
                best_score, best_container = score, parent_el
        if best_container is None or best_score < 0.65:
            # è¿›å…¥ fallbackï¼šæŒ‰ token åœ¨å…¨å±€æœç´¢
            _record_by_tokens(soup, raw_clean, paths)
            continue

        # â€”â€”â€” â‘¢ æ ‡ç­¾çº§åˆ†è¯ï¼šåœ¨ best_container å†…éƒ¨æŒ‰å…ƒç´ è¾¹ç•Œåˆ‡è¯ â€”â€”â€”
        tokens = _tokenize(raw_clean)
        token_tags = _locate_tokens_inside_container(best_container, tokens)
        if not token_tags:  # æ²¡æœ‰å…¨éƒ¨ token => Fallback
            _record_by_tokens(soup, raw_clean, paths)
            continue

        # â€”â€”â€” â‘£ æ‰¾å…¬å…±çˆ¶å…ƒç´ ä½œä¸ºæœ€ç»ˆå®šä½ â€”â€”â€”
        common_parent = _lowest_common_parent(token_tags)
        target_tag = common_parent if common_parent else best_container
        paths[get_dom_path(target_tag)].append(target_tag)

    return paths


# ----------------------------------------------------------------------------
#  è¾…åŠ©å®ç°ï¼ˆæ ‡ç­¾çº§åˆ†è¯ & å…¬å…±çˆ¶å…ƒç´ ï¼‰
# ----------------------------------------------------------------------------

def _tokenize(text: str) -> List[str]:
    """ç®€å•æŒ‰éå­—æ¯æ•°å­—åˆ†è¯ï¼Œè¿‡æ»¤ç©º tokenã€‚"""
    return [tok for tok in re.split(r"\W+", text) if tok]


def _locate_tokens_inside_container(container: Tag, tokens: List[str]) -> List[Tag]:
    """åœ¨ *container* å†…é€è¯å®šä½ï¼Œè¦æ±‚æ¯ä¸ª token éƒ½èƒ½åŒ¹é…åˆ°ç‹¬ç«‹æ ‡ç­¾ã€‚"""

    def match_tag(root: Tag, token: str) -> Optional[Tag]:
        # ä¼˜å…ˆæ‰¾æ–‡æœ¬å®Œå…¨ç­‰äº token çš„å…ƒç´ ï¼›å…¶æ¬¡å­ä¸²åŒ…å«ã€‚
        exact = root.find(lambda t: t.string and t.string.strip().lower() == token.lower())
        if exact:
            return exact
        return root.find(lambda t: t.string and token.lower() in t.string.lower())

    matches: List[Tag] = []
    for tk in tokens:
        mt = match_tag(container, tk)
        if not mt:
            return []  # æœ‰ token æœªå‘½ä¸­ï¼Œåˆ™è®¤ä¸ºå¤±è´¥
        matches.append(mt)
    return matches


def _lowest_common_parent(tags: List[Tag]) -> Optional[Tag]:
    """è¿”å›ä¸€ç»„æ ‡ç­¾çš„æœ€ä½å…¬å…±çˆ¶å…ƒç´ ã€‚è‹¥ä¸å­˜åœ¨åˆ™è¿”å› Noneã€‚"""
    if not tags:
        return None
    # å…ˆæŠŠå„è‡ªç¥–å…ˆè·¯å¾„åˆ—å‡ºæ¥ï¼ˆå«è‡ªèº«ï¼‰
    paths = []
    for t in tags:
        p: List[Tag] = []
        cur: Optional[Tag] = t
        while cur is not None:
            p.append(cur)
            cur = cur.parent  # type: ignore[assignment]
        paths.append(list(reversed(p)))

    # å¯¹æ¯”å…¬å…±å‰ç¼€
    lcp: List[Tag] = []
    for zipped in zip(*paths):
        if all(node is zipped[0] for node in zipped):
            lcp.append(zipped[0])
        else:
            break
    return lcp[-1] if lcp else None


def _record_by_tokens(soup: BeautifulSoup, raw_clean: str, paths: Dict[str, List[Tag]]):
    """Fallback é€»è¾‘ï¼šæŠŠ raw æ‹†åˆ† token ååœ¨å…¨å±€æœç´¢å¹¶è®°å½•åˆ° pathsã€‚"""
    for tk in _tokenize(raw_clean):
        tag = soup.find(lambda t: t.string and tk.lower() in t.string.lower())
        if tag:
            paths[get_dom_path(tag)].append(tag)


# ============================================================================
#  å…¶ä»–åŠŸèƒ½å‡½æ•°
# ============================================================================

def filter_paths(paths: Dict[str, List[Tag]]) -> List[Tag]:
    """ç­›é€‰å‡ºç°æ¬¡æ•°æœ€å¤šçš„è·¯å¾„å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨ï¼Œéšæœºè¿”å›è‡³å¤šä¸¤ä¸ªã€‚"""
    if not paths:
        return []
    max_occurrence = max(len(tags) for tags in paths.values())
    filtered = {p: tags for p, tags in paths.items() if len(tags) == max_occurrence}
    candidate_tags = next(iter(filtered.values()), [])
    
    # ä¼˜å…ˆé€‰æ‹©ç›¸é‚»çš„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å®Œå…¨éšæœº
    if len(candidate_tags) <= 2:
        return candidate_tags
    
    # é€‰æ‹©DOMç»“æ„ä¸Šç›¸è¿‘çš„æ ‡ç­¾
    return select_nearby_tags(candidate_tags, 2)


def select_nearby_tags(tags: List[Tag], count: int) -> List[Tag]:
    """
    ä»tagsåˆ—è¡¨ä¸­é€‰æ‹©æœ€å¤šcountä¸ªåœ¨DOMç»“æ„ä¸­å½¼æ­¤"ç›¸è¿‘"çš„æ ‡ç­¾ã€‚
    "ç›¸è¿‘"é€šè¿‡DOMè·¯å¾„çš„ç›¸ä¼¼åº¦æ¥ç¡®å®šã€‚
    """
    if len(tags) <= count:
        return tags
    
    # å¦‚æœåªéœ€è¦ä¸€ä¸ªæ ‡ç­¾ï¼Œéšæœºè¿”å›ä¸€ä¸ª
    if count == 1:
        return [random.choice(tags)]
    
    # è®¡ç®—æ‰€æœ‰æ ‡ç­¾å¯¹ä¹‹é—´çš„DOMè·¯å¾„ç›¸ä¼¼åº¦
    best_score = -1
    best_pair = None
    
    for i in range(len(tags)):
        for j in range(i+1, len(tags)):
            tag1, tag2 = tags[i], tags[j]
            path1 = get_dom_path(tag1)
            path2 = get_dom_path(tag2)
            
            # è®¡ç®—DOMè·¯å¾„çš„ç›¸ä¼¼åº¦
            similarity = _similar_ratio(path1, path2)
            
            if similarity > best_score:
                best_score = similarity
                best_pair = (tag1, tag2)
    
    # å¦‚æœæ‰¾åˆ°äº†ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€å¯¹ï¼Œè¿”å›å®ƒä»¬
    if best_pair:
        return list(best_pair)
    
    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æœ€ä½³å¯¹ï¼Œéšæœºé€‰æ‹©
    return random.sample(tags, count)


def find_parent_with_multiple_descriptions(tags: List[Tag]) -> Optional[Tag]:
    """åœ¨å€™é€‰æ ‡ç­¾ä¸­æ‰¾åˆ°æœ€ä½å…¬å…±çˆ¶å…ƒç´ ï¼Œè¦æ±‚å®ƒçš„å­èŠ‚ç‚¹ä¸­åŒ…å«æ‰€æœ‰æ ‡ç­¾æ–‡æœ¬ã€‚"""
    if not tags:
        return None
    parents = [tag.parent for tag in tags]
    while True:
        if all(parents[0] is p for p in parents):
            parent = parents[0]
            texts = [t.get_text() for t in tags]
            if all(any(txt in desc.get_text() for desc in parent.find_all()) for txt in texts):
                return parent  # type: ignore[returnâ€‘value]
        parents = [p.parent or p for p in parents]
        if all(p.name == "html" for p in parents):  # type: ignore[unionâ€‘attr]
            return None


def get_mhtml_file(file_path: str | None = None) -> Path:
    """è·å–è¦å¤„ç†çš„ MHTML æ–‡ä»¶"""
    if file_path:
        fp = Path(file_path)
    else:
        fp = next(MHTML_DIR.glob("*.mhtml"), None)  # type: ignore[assignment]
    if not fp or not fp.exists():
        raise FileNotFoundError(f"Cannot find relevant MHTML file: {fp}")
    return fp


async def get_html_content(file_path: Path) -> str:
    """ä½¿ç”¨ Playwright å¼‚æ­¥ API åŠ è½½ MHTML æ–‡ä»¶å¹¶è·å– HTML å†…å®¹"""
    playwright = await async_playwright().start()
    browser = await playwright.chromium.launch(headless=True)
    page = await browser.new_page()
    await page.goto(file_path.as_uri())
    await asyncio.sleep(5)  # ç®€æ˜“ç­‰å¾…
    html_content = await page.content()
    await browser.close()
    await playwright.stop()
    return html_content


def save_beautiful_soup_content(beautiful_soup: List[Dict], output_dir: Path = None) -> bool:
    """
    ä¿å­˜æå–çš„å†…å®¹åˆ°JSONæ–‡ä»¶
    """
    if beautiful_soup:
        if output_dir is None:
            output_dir = EXTRACTOR_DIR
        out_path = output_dir / "BeautifulSoup_Content.json"
        with open(out_path, "w", encoding="utf-8") as jf:
            json.dump(beautiful_soup, jf, ensure_ascii=False, indent=4)
        print(f"å·²å†™å…¥ JSONï¼š{out_path}")
        return True
    else:
        print("å…¬å…±çˆ¶èŠ‚ç‚¹ä¸‹æ²¡æœ‰æœ‰æ•ˆå­èŠ‚ç‚¹å†…å®¹ã€‚")
        return False


def load_item_info(key: str = 'item') -> List[str]:
    """
    ä»item_info.jsonåŠ è½½ä¿¡æ¯
    keyå¯ä»¥æ˜¯'item'(å•†å“åç§°)æˆ–'price'(ä»·æ ¼)
    """
    product_names = []
    item_info_path = PROJECT_ROOT / 'item_info.json'
    
    if not item_info_path.exists():
        item_info_path = EXTRACTOR_DIR / 'item_info.json'
    
    if not item_info_path.exists():
        print(f"æ‰¾ä¸åˆ°item_info.jsonæ–‡ä»¶")
        return []
    
    try:
        with open(item_info_path, 'r', encoding='utf-8') as f:
            try:
                item_data = json.load(f)
                product_names = [str(item.get(key, '')) for item in item_data if key in item]
                print(f"æ‰¾åˆ°{len(product_names)}ä¸ª{key}ä¿¡æ¯")
                return product_names
            except json.JSONDecodeError as e:
                print(f"è§£æ{item_info_path}å¤±è´¥: {str(e)}")
                return []
    except Exception as e:
        print(f"è¯»å–{item_info_path}å¤±è´¥: {str(e)}")
        return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æµç¨‹å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def process_tag_location(product_names: List[str], file_path: str | None = None) -> bool:
    """
    é€šç”¨çš„æ ‡ç­¾å®šä½å¤„ç†æµç¨‹(å¼‚æ­¥ç‰ˆæœ¬)
    """
    try:
        # 1. è·å–MHTMLæ–‡ä»¶
        fp = get_mhtml_file(file_path)
        print(f"å¤„ç†MHTMLæ–‡ä»¶: {fp}")
        
        # 2. ç”¨PlaywrightåŠ è½½é¡µé¢è·å–HTMLå†…å®¹
        html_content = await get_html_content(fp)
        
        # 3. è§£æDOMï¼Œå®šä½äº§å“åç§°å¯¹åº”çš„æ ‡ç­¾è·¯å¾„
        soup = BeautifulSoup(html_content, "html.parser")
        paths = get_item_paths(soup, product_names)
        print(f"æ‰¾åˆ°çš„è·¯å¾„æ•°é‡: {len(paths)}")
        
        # 4. ç­›é€‰å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ ‡ç­¾
        majority_tags = filter_paths(paths)
        if not majority_tags:
            print("è­¦å‘Š: æ²¡æœ‰åŒ¹é…åˆ°æœ‰æ•ˆçš„æ ‡ç­¾ï¼Œè·³è¿‡åç»­å¤„ç†ã€‚")
            return False
        print(f"é€‰å–äº†{len(majority_tags)}ä¸ªæ ‡ç­¾")
        
        # 5. æ‰¾æœ€ä½å…¬å…±çˆ¶èŠ‚ç‚¹
        common_parent = find_parent_with_multiple_descriptions(majority_tags)
        if not common_parent:
            print("è­¦å‘Š: æœªæ‰¾åˆ°åŒ…å«æ‰€æœ‰æè¿°çš„å…¬å…±çˆ¶å…ƒç´ ã€‚")
            return False
        
        # 6. éå†å…¬å…±çˆ¶èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ï¼Œæå–å†…å®¹å¹¶å†™å…¥JSON
        beautiful_soup = []
        for idx, child in enumerate(common_parent.children, start=1):
            if getattr(child, "prettify", None):
                content = child.prettify().strip()
                if content:
                    beautiful_soup.append({
                        "Order": idx,
                        "Content": content
                    })
        
        if beautiful_soup:
            out_path = EXTRACTOR_DIR / "BeautifulSoup_Content.json"
            with open(out_path, "w", encoding="utf-8") as jf:
                json.dump(beautiful_soup, jf, ensure_ascii=False, indent=4)
            print(f"å·²å†™å…¥JSON: {out_path}")
            return True
        else:
            print("è­¦å‘Š: å…¬å…±çˆ¶èŠ‚ç‚¹ä¸‹æ²¡æœ‰æœ‰æ•ˆå­èŠ‚ç‚¹å†…å®¹ã€‚")
            return False
    
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"é”™è¯¯: å¤„ç†æ ‡ç­¾å®šä½æ—¶å‡ºé”™: {str(e)}")
        print(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
        return False


async def process_name_tag_location(file_path: str | None = None) -> bool:
    """
    å¤„ç†å•†å“åç§°æ ‡ç­¾å®šä½
    """
    print("å¼€å§‹å¤„ç†å•†å“åç§°æ ‡ç­¾å®šä½...")
    
    # ä»item_info.jsonåŠ è½½å•†å“åç§°
    try:
        product_names = load_item_info(key='item')
        
        if not product_names:
            print("é”™è¯¯: æœªæ‰¾åˆ°å•†å“åç§°ä¿¡æ¯ï¼Œè¯·å…ˆè¿è¡ŒOCRåç§°æå–")
            return False
        
        # æ‰§è¡Œæ ‡ç­¾å®šä½å¤„ç†
        try:
            result = await process_tag_location(product_names, file_path)
            if result:
                print("å•†å“åç§°æ ‡ç­¾å®šä½å¤„ç†å®Œæˆ")
            else:
                print("å•†å“åç§°æ ‡ç­¾å®šä½å¤„ç†å¤±è´¥")
            return result
                
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            print(f"é”™è¯¯: å•†å“åç§°æ ‡ç­¾å®šä½å¤„ç†å‡ºé”™: {str(e)}")
            print(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
            return False
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"é”™è¯¯: åŠ è½½å•†å“åç§°ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        print(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
        return False


async def process_price_tag_location(file_path: str | None = None) -> bool:
    """
    å¤„ç†å•†å“ä»·æ ¼æ ‡ç­¾å®šä½
    """
    print("å¼€å§‹å¤„ç†å•†å“ä»·æ ¼æ ‡ç­¾å®šä½...")
    
    # ä»item_info.jsonåŠ è½½ä»·æ ¼ä¿¡æ¯
    try:
        price_info = load_item_info(key='price')
        
        if not price_info:
            print("é”™è¯¯: æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯ï¼Œè¯·å…ˆè¿è¡ŒOCRä»·æ ¼æå–")
            return False
        
        # æ‰§è¡Œæ ‡ç­¾å®šä½å¤„ç†
        try:
            result = await process_tag_location(price_info, file_path)
            if result:
                print("å•†å“ä»·æ ¼æ ‡ç­¾å®šä½å¤„ç†å®Œæˆ")
            else:
                print("å•†å“ä»·æ ¼æ ‡ç­¾å®šä½å¤„ç†å¤±è´¥")
            return result
                
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            print(f"é”™è¯¯: å•†å“ä»·æ ¼æ ‡ç­¾å®šä½å¤„ç†å‡ºé”™: {str(e)}")
            print(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
            return False
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"é”™è¯¯: åŠ è½½ä»·æ ¼ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        print(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
        return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æµ‹è¯•åŠŸèƒ½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def test_with_real_data():
    """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½"""
    print("=" * 60)
    print(f"æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨çœŸå® MHTML æ–‡ä»¶å’Œ item_info.json")
    print("=" * 60)
    
    # 1. ä» item_info.json è¯»å–å•†å“åç§°
    print("1. ä» item_info.json è¯»å–å•†å“åç§°...")
    product_names = load_item_info(key='item')
    
    if not product_names:
        print("é”™è¯¯: æœªæ‰¾åˆ°å•†å“åç§°ä¿¡æ¯")
        return False
    
    print(f"   æˆåŠŸè¯»å– {len(product_names)} ä¸ªå•†å“åç§°")
    # æ˜¾ç¤ºå‰å‡ ä¸ªå•†å“åç§°
    for i, name in enumerate(product_names[:5], 1):
        print(f"   {i}. {name}")
    if len(product_names) > 5:
        print(f"   ... è¿˜æœ‰ {len(product_names) - 5} ä¸ªå•†å“")
    
    # 2. ä» mhtml_output è¯»å– MHTML æ–‡ä»¶
    print("\n2. ä» mhtml_output è¯»å– MHTML æ–‡ä»¶...")
    try:
        mhtml_file = get_mhtml_file()
        print(f"   æ‰¾åˆ° MHTML æ–‡ä»¶: {mhtml_file}")
    except FileNotFoundError as e:
        print(f"   é”™è¯¯: {e}")
        return False
    
    # 3. ä½¿ç”¨ Playwright è·å– HTML å†…å®¹
    print("\n3. ä½¿ç”¨ Playwright åŠ è½½é¡µé¢è·å– HTML å†…å®¹...")
    try:
        html_content = await get_html_content(mhtml_file)
        print(f"   æˆåŠŸè·å– HTML å†…å®¹ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
    except Exception as e:
        print(f"   é”™è¯¯: è·å– HTML å†…å®¹å¤±è´¥: {e}")
        return False
    
    # 4. è§£æ HTML å¹¶æ‰§è¡Œæ ‡ç­¾åŒ¹é…
    soup = BeautifulSoup(html_content, "html.parser")    
    # æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
    print("\n4. æµ‹è¯• get_item_paths å‡½æ•°...")
    paths = get_item_paths(soup, product_names)
    print(f"   æ‰¾åˆ°è·¯å¾„æ•°é‡: {len(paths)}")
    for path, tags in paths.items():
        print(f"   è·¯å¾„: {path} -> {len(tags)} ä¸ªæ ‡ç­¾")
    
    print("\n5. æµ‹è¯• filter_paths å‡½æ•°...")
    majority_tags = filter_paths(paths)
    print(f"   ç­›é€‰å‡º {len(majority_tags)} ä¸ªä¸»è¦æ ‡ç­¾")
    
    if not majority_tags:
        print("   è­¦å‘Š: æ²¡æœ‰åŒ¹é…åˆ°æœ‰æ•ˆçš„æ ‡ç­¾")
        return False
    
    print("\n6. æµ‹è¯• find_parent_with_multiple_descriptions å‡½æ•°...")
    common_parent = find_parent_with_multiple_descriptions(majority_tags)
    if common_parent:
        print(f"   æ‰¾åˆ°å…¬å…±çˆ¶å…ƒç´ : {common_parent.name}")
        print(f"   çˆ¶å…ƒç´ å†…å®¹é¢„è§ˆ: {common_parent.get_text()[:100]}...")
    else:
        print("   æœªæ‰¾åˆ°å…¬å…±çˆ¶å…ƒç´ ")
        return False
    
    print("\n7. æå–å¹¶ä¿å­˜å†…å®¹åˆ° test_output.json...")
    beautiful_soup = []
    for idx, child in enumerate(common_parent.children, start=1):
        if getattr(child, "prettify", None):
            content = child.prettify().strip()
            if content:
                beautiful_soup.append({
                    "Order": idx,
                    "Content": content
                })
    
    print(f"   æå–å‡º {len(beautiful_soup)} ä¸ªå­å…ƒç´ ")
    
    # ä¿å­˜æµ‹è¯•ç»“æœåˆ° test_output.json
    test_output_path = PROJECT_ROOT / "test_output.json"
    with open(test_output_path, "w", encoding="utf-8") as f:
        json.dump(beautiful_soup, f, ensure_ascii=False, indent=4)
    print(f"   æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_output_path}")
    
    print("\næµ‹è¯•å®Œæˆï¼æ ‡ç­¾åŒ¹é…æˆåŠŸï¼Œç»“æœå·²ä¿å­˜åˆ° test_output.json")
    return True


def test_with_sample_data():
    """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•ï¼Œä»mhtml_outputè¯»å–æ–‡ä»¶ï¼Œå¤„ç†item_info.jsonæ•°æ®ï¼Œè¾“å‡ºåˆ°BeautifulSoup_Content.json"""
    print("=" * 60)
    print("æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨çœŸå®æ•°æ®")
    print("=" * 60)
    
    # 1. ä» item_info.json è¯»å–å•†å“åç§°
    print("1. ä» item_info.json è¯»å–å•†å“åç§°...")
    product_names = load_item_info(key='item')
    
    if not product_names:
        print("é”™è¯¯: æœªæ‰¾åˆ°å•†å“åç§°ä¿¡æ¯")
        return False
    
    print(f"   æˆåŠŸè¯»å– {len(product_names)} ä¸ªå•†å“åç§°")
    for i, name in enumerate(product_names[:3], 1):
        print(f"   {i}. {name}")
    if len(product_names) > 3:
        print(f"   ... è¿˜æœ‰ {len(product_names) - 3} ä¸ªå•†å“")
    
    # 2. ä» mhtml_output è¯»å– MHTML æ–‡ä»¶
    print("\n2. ä» mhtml_output è¯»å– MHTML æ–‡ä»¶...")
    try:
        mhtml_file = get_mhtml_file()
        print(f"   æ‰¾åˆ° MHTML æ–‡ä»¶: {mhtml_file}")
    except FileNotFoundError as e:
        print(f"   é”™è¯¯: {e}")
        return False
    
    # 3. ä½¿ç”¨ Playwright è·å– HTML å†…å®¹
    print("\n3. ä½¿ç”¨ Playwright åŠ è½½é¡µé¢è·å– HTML å†…å®¹...")
    try:
        html_content = asyncio.get_event_loop().run_until_complete(get_html_content(mhtml_file))
        print(f"   æˆåŠŸè·å– HTML å†…å®¹ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
    except Exception as e:
        print(f"   é”™è¯¯: è·å– HTML å†…å®¹å¤±è´¥: {e}")
        return False
    
    # 4. è§£æ HTML å¹¶æ‰§è¡Œæ ‡ç­¾åŒ¹é…
    soup = BeautifulSoup(html_content, "html.parser")    
    print("\n4. æ‰§è¡Œæ ‡ç­¾åŒ¹é…...")
    paths = get_item_paths(soup, product_names)
    print(f"   æ‰¾åˆ°è·¯å¾„æ•°é‡: {len(paths)}")
    
    # 5. ç­›é€‰å‡ºä¸»è¦æ ‡ç­¾
    majority_tags = filter_paths(paths)
    print(f"   ç­›é€‰å‡º {len(majority_tags)} ä¸ªä¸»è¦æ ‡ç­¾")
    
    if not majority_tags:
        print("   è­¦å‘Š: æ²¡æœ‰åŒ¹é…åˆ°æœ‰æ•ˆçš„æ ‡ç­¾")
        return False
    
    # 6. æ‰¾åˆ°å…¬å…±çˆ¶å…ƒç´ 
    common_parent = find_parent_with_multiple_descriptions(majority_tags)
    if common_parent:
        print(f"   æ‰¾åˆ°å…¬å…±çˆ¶å…ƒç´ : {common_parent.name}")
    else:
        print("   æœªæ‰¾åˆ°å…¬å…±çˆ¶å…ƒç´ ")
        return False
    
    # 7. æå–å¹¶ä¿å­˜å†…å®¹åˆ° BeautifulSoup_Content.json
    beautiful_soup = []
    for idx, child in enumerate(common_parent.children, start=1):
        if getattr(child, "prettify", None):
            content = child.prettify().strip()
            if content:
                beautiful_soup.append({
                    "Order": idx,
                    "Content": content
                })
    
    print(f"   æå–å‡º {len(beautiful_soup)} ä¸ªå­å…ƒç´ ")
    
    # ä¿å­˜åˆ°EXTRACTOR_DIR/BeautifulSoup_Content.json
    output_path = EXTRACTOR_DIR / "BeautifulSoup_Content.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(beautiful_soup, f, ensure_ascii=False, indent=4)
    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    print("\nå¤„ç†å®Œæˆï¼")
    return True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main_async():
    parser = argparse.ArgumentParser(description="Tag locating test tool")
    parser.add_argument("--type", choices=["name", "price", "test", "real"], default="real", 
                        help="Processing type: name (product name), price, test (sample data test), or real (use real data)")
    parser.add_argument("--filepath", default=None,
                        help="Path to the MHTML file to process (optional, defaults to the latest file in mhtml_output)")
    args = parser.parse_args()
    
    if args.type == "test":
        test_with_sample_data()
    elif args.type == "real":
        await test_with_real_data()
    elif args.type == "name":
        await process_name_tag_location(args.filepath)
    else:
        await process_price_tag_location(args.filepath)


def main():
    """åŒæ­¥å…¥å£å‡½æ•°"""
    print("Tag Locating æµ‹è¯•ç¨‹åº")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  python test_tag_locating.py --type real     # ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯• (é»˜è®¤)")
    print("  python test_tag_locating.py --type test     # è¿è¡Œç¤ºä¾‹æ•°æ®æµ‹è¯•")
    print("  python test_tag_locating.py --type name     # å¤„ç†å•†å“åç§°å®šä½")
    print("  python test_tag_locating.py --type price    # å¤„ç†å•†å“ä»·æ ¼å®šä½")
    print("  python test_tag_locating.py --type real --filepath path/to/file.mhtml")
    print("")
    
    asyncio.run(main_async())


if __name__ == "__main__":
    main()
