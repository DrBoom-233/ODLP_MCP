"""
æ ‡ç­¾å®šä½æ¨¡å— - æä¾›ä» MHTML æ–‡ä»¶ä¸­å®šä½å•†å“åç§°å’Œä»·æ ¼æ ‡ç­¾çš„åŠŸèƒ½ã€‚

é‡å†™è¦ç‚¹ï¼ˆ2025â€‘06â€‘17ï¼‰ï¼š
1. **æ–°å¢ä¸‰æ®µå¼å®šä½ç­–ç•¥**ï¼šæ¨¡ç³Šå®šä½ âœ æ ‡ç­¾çº§åˆ†è¯ âœ é€è¯ç²¾ç¡®å®šä½ã€‚
2. **æ ¸å¿ƒé€»è¾‘å…¨éƒ¨å°è£…åœ¨ `get_item_paths`**ï¼Œå¯¹å¤–å‡½æ•°ç­¾åä¸å˜ï¼ŒServer ä¾§æ— éœ€æ”¹åŠ¨ã€‚
3. ä»ä¿ç•™åŸæœ‰ BeautifulSoup Fallbackï¼Œä¿è¯åœ¨æç«¯ç»“æ„ä¸‹ä¹Ÿèƒ½äº§å‡ºç»“æœã€‚
"""

from __future__ import annotations

import asyncio
import io
import json
import os
import random
import re
import sys
import time
import argparse
from collections import defaultdict
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from bs4 import BeautifulSoup, Tag
from playwright.async_api import async_playwright, Page
from playwright.sync_api import sync_playwright  # ä¿ç•™åŒæ­¥ç‰ˆæ¥å£ï¼Œéƒ¨åˆ† CLI è°ƒç”¨ä»ä¾èµ–

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç¼–ç å…¼å®¹ï¼šè§£å†³ Windows ä¸‹ä¸­æ–‡è¾“å‡ºä¹±ç 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç›®å½•å¸¸é‡ï¼šåŸºäºè„šæœ¬ä½ç½®å®šä½é¡¹ç›®æ ¹å’Œ mhtml è¾“å‡ºç›®å½•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THIS_DIR = Path(__file__).resolve().parent             # extractor/
PROJECT_ROOT = THIS_DIR.parent                         # mcpâ€‘project æ ¹ç›®å½•
MHTML_DIR = PROJECT_ROOT / "mhtml_output"              # mhtml_output ä¸ extractor åŒçº§

# ============================================================================
# ğŸ”‘  è¾…åŠ©å·¥å…·
# ============================================================================

def _escape_regex(text: str) -> str:
    """å¯¹æ­£åˆ™å…ƒå­—ç¬¦è½¬ä¹‰"""
    return re.escape(text)


def _similar_ratio(a: str, b: str) -> float:
    """å¤§å°å†™æ— å…³çš„ SequenceMatcher ç›¸ä¼¼åº¦ [0â€‘1]"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def get_dom_path(tag: Tag) -> str:
    """è·å–ä»å½“å‰æ ‡ç­¾åˆ°æ ¹èŠ‚ç‚¹çš„ DOM è·¯å¾„ï¼ˆtagName ä¸²è”ï¼‰ã€‚"""
    segments = []
    while tag is not None:
        segments.append(tag.name)
        tag = tag.parent  # type: ignore[attrâ€‘defined]
    return " > ".join(reversed(segments))


# ----------------------------------------------------------------------------
# æ ¸å¿ƒ âœ¨ get_item_paths âœ¨ï¼šä¸‰æ®µå¼å®šä½ç­–ç•¥å®ç°
# ----------------------------------------------------------------------------

def get_item_paths(soup: BeautifulSoup, product_names: List[str]) -> Dict[str, List[Tag]]:
    """æ ¹æ® *product_names* åœ¨ DOM ä¸­å®šä½æ ‡ç­¾ï¼ˆæˆ–å…¶å…¬å…±çˆ¶å…ƒç´ ï¼‰çš„è·¯å¾„ã€‚

    **å®ç°é€»è¾‘**
    1. å°è¯• *ç²¾ç¡®/åŒ…å«* åŒ¹é…æ•´ä¸ªå­—ç¬¦ä¸²ï¼›æˆåŠŸåˆ™è®°å½•ã€‚
    2. è‹¥å¤±è´¥ â†’ **æ¨¡ç³Š**ï¼šä½¿ç”¨ç›¸ä¼¼åº¦ > 0.65 çš„å…ƒç´ åšç²—å®šä½ï¼ˆç²—å®¹å™¨ï¼‰ã€‚
    3. åœ¨ç²—å®¹å™¨å†…éƒ¨ **æ ‡ç­¾çº§åˆ†è¯**ï¼šæŒ‰å…ƒç´ è¾¹ç•Œåˆ‡è¯ï¼›å¯¹é½åŸå­—ç¬¦ä¸²åˆ‡åˆ† tokenã€‚
    4. å¯¹æ¯ä¸ª token å†åš **é€è¯ç²¾ç¡®å®šä½**ï¼›æ‰¾æœ€ä½å…¬å…±çˆ¶å…ƒç´ ä½œä¸ºæœ€ç»ˆæ ‡ç­¾ã€‚
    5. Fallbackï¼šä»æ— æ³•å®šä½ï¼Œåˆ™æ‹†è¯åç›´æ¥åœ¨æ–‡æ¡£çº§æœç´¢ï¼Œæ¯è¯ç‹¬ç«‹å¤„ç†ã€‚
    """

    paths: Dict[str, List[Tag]] = defaultdict(list)

    # é¢„ç¼–è¯‘å¸¸ç”¨å‡½æ•°
    def exact_or_contains(txt: str) -> Optional[Tag]:
        # å®Œæ•´åŒ¹é…
        exact = soup.find(lambda t: t.string and t.string.strip().lower() == txt.lower())
        if exact:
            return exact
        # å­ä¸²åŒ…å«
        return soup.find(lambda t: t.string and txt.lower() in t.string.lower())

    for raw in product_names:
        raw_clean = raw.strip()
        if not raw_clean:
            continue

        # â€”â€”â€” â‘  ç²¾ç¡® / åŒ…å«åŒ¹é… â€”â€”â€”
        tag = exact_or_contains(raw_clean)
        if tag:
            paths[get_dom_path(tag)].append(tag)
            continue  # âœ… ç›´æ¥æ‰¾åˆ°ï¼Œè·³è¿‡åç»­

        # â€”â€”â€” â‘¡ æ¨¡ç³Šå®šä½ï¼šæ‰¾ç›¸ä¼¼åº¦æœ€é«˜çš„å…ƒç´ ä½œä¸ºâ€œç²—å®¹å™¨â€ â€”â€”â€”
        # å…ˆç²—æš´æ‹¿æ‰€æœ‰åŒ…å«å•è¯çš„å…ƒç´ ï¼ˆé˜²æ­¢å…¨å±€éå†è€—æ—¶ï¼‰
        word_pat = re.compile(_escape_regex(raw_clean.split()[0]), re.I)
        candidates = [t for t in soup.find_all(string=word_pat) if isinstance(t, str)]
        best_container: Optional[Tag] = None
        best_score = 0.0
        for text_node in candidates:
            parent_el = text_node.parent  # type: ignore[assignment]
            text_val = text_node.strip()
            score = _similar_ratio(text_val, raw_clean)
            if score > best_score:
                best_score, best_container = score, parent_el
        if best_container is None or best_score < 0.65:
            # è¿›å…¥ fallbackï¼šæŒ‰ token åœ¨å…¨å±€æœç´¢
            _record_by_tokens(soup, raw_clean, paths)
            continue

        # â€”â€”â€” â‘¢ æ ‡ç­¾çº§åˆ†è¯ï¼šåœ¨ best_container å†…éƒ¨æŒ‰å…ƒç´ è¾¹ç•Œåˆ‡è¯ â€”â€”â€”
        tokens = _tokenize(raw_clean)
        token_tags = _locate_tokens_inside_container(best_container, tokens)
        if not token_tags:  # æ²¡æœ‰å…¨éƒ¨ token => Fallback
            _record_by_tokens(soup, raw_clean, paths)
            continue

        # â€”â€”â€” â‘£ æ‰¾å…¬å…±çˆ¶å…ƒç´ ä½œä¸ºæœ€ç»ˆå®šä½ â€”â€”â€”
        common_parent = _lowest_common_parent(token_tags)
        target_tag = common_parent if common_parent else best_container
        paths[get_dom_path(target_tag)].append(target_tag)

    return paths


# ----------------------------------------------------------------------------
#  è¾…åŠ©å®ç°ï¼ˆæ ‡ç­¾çº§åˆ†è¯ & å…¬å…±çˆ¶å…ƒç´ ï¼‰
# ----------------------------------------------------------------------------

def _tokenize(text: str) -> List[str]:
    """ç®€å•æŒ‰éå­—æ¯æ•°å­—åˆ†è¯ï¼Œè¿‡æ»¤ç©º tokenã€‚"""
    return [tok for tok in re.split(r"\W+", text) if tok]


def _locate_tokens_inside_container(container: Tag, tokens: List[str]) -> List[Tag]:
    """åœ¨ *container* å†…é€è¯å®šä½ï¼Œè¦æ±‚æ¯ä¸ª token éƒ½èƒ½åŒ¹é…åˆ°ç‹¬ç«‹æ ‡ç­¾ã€‚"""

    def match_tag(root: Tag, token: str) -> Optional[Tag]:
        # ä¼˜å…ˆæ‰¾æ–‡æœ¬å®Œå…¨ç­‰äº token çš„å…ƒç´ ï¼›å…¶æ¬¡å­ä¸²åŒ…å«ã€‚
        exact = root.find(lambda t: t.string and t.string.strip().lower() == token.lower())
        if exact:
            return exact
        return root.find(lambda t: t.string and token.lower() in t.string.lower())

    matches: List[Tag] = []
    for tk in tokens:
        mt = match_tag(container, tk)
        if not mt:
            return []  # æœ‰ token æœªå‘½ä¸­ï¼Œåˆ™è®¤ä¸ºå¤±è´¥
        matches.append(mt)
    return matches


def _lowest_common_parent(tags: List[Tag], ctx=None) -> Optional[Tag]:
    """è¿”å›ä¸€ç»„æ ‡ç­¾çš„æœ€ä½å…¬å…±çˆ¶å…ƒç´ ã€‚è‹¥ä¸å­˜åœ¨åˆ™è¿”å› Noneã€‚"""
    if not tags:
        return None
        
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šè¾“å‡ºå‚ä¸æŸ¥æ‰¾çš„æ ‡ç­¾ä¿¡æ¯
    if ctx:
        ctx.info(f"\n===== å¼€å§‹æŸ¥æ‰¾ {len(tags)} ä¸ªæ ‡ç­¾çš„æœ€å°å…¬å…±çˆ¶å…ƒç´  =====")
        for i, tag in enumerate(tags, 1):
            ctx.info(f"æ ‡ç­¾ {i}: <{tag.name}> - æ–‡æœ¬: {tag.get_text().strip()[:50]}")
            ctx.info(f"DOMè·¯å¾„: {get_dom_path(tag)}")
    
    # å…ˆæŠŠå„è‡ªç¥–å…ˆè·¯å¾„åˆ—å‡ºæ¥ï¼ˆå«è‡ªèº«ï¼‰
    paths = []
    for t in tags:
        p: List[Tag] = []
        cur: Optional[Tag] = t
        while cur is not None:
            p.append(cur)
            cur = cur.parent  # type: ignore[assignment]
        paths.append(list(reversed(p)))

    # å¯¹æ¯”å…¬å…±å‰ç¼€
    lcp: List[Tag] = []
    for zipped in zip(*paths):
        if all(node is zipped[0] for node in zipped):
            lcp.append(zipped[0])
        else:
            break
    
    result = lcp[-1] if lcp else None
  
    return result


def _record_by_tokens(soup: BeautifulSoup, raw_clean: str, paths: Dict[str, List[Tag]]):
    """Fallback é€»è¾‘ï¼šæŠŠ raw æ‹†åˆ† token ååœ¨å…¨å±€æœç´¢å¹¶è®°å½•åˆ° pathsã€‚"""
    for tk in _tokenize(raw_clean):
        tag = soup.find(lambda t: t.string and tk.lower() in t.string.lower())
        if tag:
            paths[get_dom_path(tag)].append(tag)


# ============================================================================
#  å…¶ä½™åŸæœ‰ä»£ç åŸºæœ¬ä¿æŒ **ä¸å˜**
#  Â· get_mhtml_file
#  Â· get_html_content
#  Â· filter_paths, find_parent_with_multiple_descriptions
#  Â· process_* ç³»åˆ—æ¥å£
# ============================================================================

# ä»¥ä¸‹å†…å®¹ä»æ—§å®ç°æ‹·è´ï¼Œä»…åˆ å»ä¸å¿…è¦ importï¼Œé€»è¾‘ä¿æŒåŸçŠ¶ã€‚

def select_nearby_tags(tags: List[Tag], count: int) -> List[Tag]:
    """
    ä»tagsåˆ—è¡¨ä¸­é€‰æ‹©æœ€å¤šcountä¸ªåœ¨DOMç»“æ„ä¸­å½¼æ­¤"ç›¸è¿‘"çš„æ ‡ç­¾ã€‚
    "ç›¸è¿‘"é€šè¿‡DOMè·¯å¾„çš„ç›¸ä¼¼åº¦æ¥ç¡®å®šã€‚
    """
    if len(tags) <= count:
        return tags
    
    # å¦‚æœåªéœ€è¦ä¸€ä¸ªæ ‡ç­¾ï¼Œéšæœºè¿”å›ä¸€ä¸ª
    if count == 1:
        return [random.choice(tags)]
    
    # è®¡ç®—æ‰€æœ‰æ ‡ç­¾å¯¹ä¹‹é—´çš„DOMè·¯å¾„ç›¸ä¼¼åº¦
    best_score = -1
    best_pair = None
    
    for i in range(len(tags)):
        for j in range(i+1, len(tags)):
            tag1, tag2 = tags[i], tags[j]
            path1 = get_dom_path(tag1)
            path2 = get_dom_path(tag2)
            
            # è®¡ç®—DOMè·¯å¾„çš„ç›¸ä¼¼åº¦
            similarity = _similar_ratio(path1, path2)
            
            if similarity > best_score:
                best_score = similarity
                best_pair = (tag1, tag2)
    
    # å¦‚æœæ‰¾åˆ°äº†ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€å¯¹ï¼Œè¿”å›å®ƒä»¬
    if best_pair:
        return list(best_pair)
    
    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æœ€ä½³å¯¹ï¼Œéšæœºé€‰æ‹©
    return random.sample(tags, count)

def filter_paths(paths: Dict[str, List[Tag]]) -> List[Tag]:
    """ç­›é€‰å‡ºç°æ¬¡æ•°æœ€å¤šçš„è·¯å¾„å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨ï¼Œæ™ºèƒ½è¿”å›è‡³å¤šä¸¤ä¸ªã€‚"""
    if not paths:
        return []
    max_occurrence = max(len(tags) for tags in paths.values())
    filtered = {p: tags for p, tags in paths.items() if len(tags) == max_occurrence}
    candidate_tags = next(iter(filtered.values()), [])
    
    # ä¼˜å…ˆé€‰æ‹©ç›¸é‚»çš„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å®Œå…¨éšæœº
    if len(candidate_tags) <= 2:
        return candidate_tags
    
    # é€‰æ‹©DOMç»“æ„ä¸Šç›¸è¿‘çš„æ ‡ç­¾
    return select_nearby_tags(candidate_tags, 2)


def find_parent_with_multiple_descriptions(tags: List[Tag], ctx=None) -> Optional[Tag]:
    """
    åœ¨å€™é€‰æ ‡ç­¾ä¸­æ‰¾åˆ°æœ€ä½å…¬å…±çˆ¶å…ƒç´ ï¼ˆåŸºäº DOM è·¯å¾„ï¼‰ï¼Œ
    è¦æ±‚å®ƒçš„å­èŠ‚ç‚¹ä¸­åŒ…å«æ‰€æœ‰æ ‡ç­¾æ–‡æœ¬ã€‚
    """
    if not tags:
        return None
    
    # ä¿ç•™è¾“å…¥æ ‡ç­¾ä¿¡æ¯çš„è°ƒè¯•è¾“å‡º
    if ctx:
        ctx.info(f"\n===== å¼€å§‹æŸ¥æ‰¾ {len(tags)} ä¸ªæ ‡ç­¾çš„æœ€ä½å…¬å…±çˆ¶å…ƒç´  =====")
        for i, tag in enumerate(tags, 1):
            ctx.info(f"è¾“å…¥æ ‡ç­¾ {i}: <{tag.name}> - æ–‡æœ¬: {tag.get_text().strip()[:50]}")
            ctx.info(f"  DOMè·¯å¾„: {get_dom_path(tag)}")
    
    parents = [tag.parent for tag in tags]
    # if ctx:
    #     ctx.info(f"\næ­£åœ¨æ£€æŸ¥ç›´æ¥çˆ¶å…ƒç´ ...")
    #     for i, parent in enumerate(parents, 1):
    #         ctx.info(f"çˆ¶å…ƒç´  {i}: <{parent.name}> - è·¯å¾„: {get_dom_path(parent)}")
    
    level = 1
    while True:
        # æ”¹ç”¨ DOM è·¯å¾„è¿›è¡Œæ¯”è¾ƒ
        ref_path = get_dom_path(parents[0])
        if all(get_dom_path(p) == ref_path for p in parents):
            parent = parents[0]
            texts = [t.get_text() for t in tags]
            
            # if ctx:
            #     ctx.info(f"\nåœ¨ç¬¬ {level} å±‚æ‰¾åˆ°ç›¸åŒDOMè·¯å¾„: {ref_path}")
            #     ctx.info(f"æ­£åœ¨æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰æ ‡ç­¾æ–‡æœ¬...")
            
            if all(any(txt in desc.get_text() for desc in parent.find_all()) for txt in texts):
                if ctx:
                    ctx.info(f"âœ“ æ‰¾åˆ°åŒ…å«æ‰€æœ‰æ–‡æœ¬çš„çˆ¶å…ƒç´ : <{parent.name}> - è·¯å¾„: {get_dom_path(parent)}")
                    ctx.info(f"  å†…å®¹é¢„è§ˆ: {parent.get_text().strip()[:100]}...")
                return parent
            # elif ctx:
            #     ctx.info(f"âœ— çˆ¶å…ƒç´  <{parent.name}> ä¸åŒ…å«æ‰€æœ‰æ ‡ç­¾æ–‡æœ¬ï¼Œç»§ç»­å‘ä¸ŠæŸ¥æ‰¾")
        
        # ç»§ç»­å¾€ä¸Šæ‰¾
        level += 1
        parents = [p.parent or p for p in parents]
        # if ctx:
        #     ctx.info(f"\næŸ¥æ‰¾ç¬¬ {level} å±‚çˆ¶å…ƒç´ ...")
        
        if all(p.name == "html" for p in parents):
            if ctx:
                ctx.info("å·²åˆ°è¾¾HTMLæ ¹èŠ‚ç‚¹ï¼Œæœªæ‰¾åˆ°åŒ…å«æ‰€æœ‰æ–‡æœ¬çš„çˆ¶å…ƒç´ ")
            return None


# get_mhtml_file, get_html_content, save_beautiful_soup_content, load_item_info,
# process_tag_location, process_name_tag_location, process_price_tag_location,
# CLI éƒ¨åˆ†å‡ä¿æŒä¸å˜ï¼Œç›´æ¥ä»æ—§æ–‡ä»¶ copy è¿‡æ¥ï¼ˆç•¥ï¼‰ã€‚

from typing import Tuple  # éœ€è¦åœ¨åé¢ç»§ç»­ä½¿ç”¨

# â€”â€” ä»¥ä¸‹æ•´æ®µç›´æ¥ä¿ç•™æ—§å®ç° â€”â€”

async def get_mhtml_file(file_path: str | None = None) -> Path:
    """è·å–è¦å¤„ç†çš„ MHTML æ–‡ä»¶"""
    if file_path:
        fp = Path(file_path)
    else:
        fp = next(MHTML_DIR.glob("*.mhtml"), None)  # type: ignore[assignment]
    if not fp or not fp.exists():
        raise FileNotFoundError(f"Cannot find relevant MHTML file: {fp}")
    return fp


async def get_html_content(file_path: Path) -> str:
    """ä½¿ç”¨ Playwright å¼‚æ­¥ API åŠ è½½ MHTML æ–‡ä»¶å¹¶è·å– HTML å†…å®¹"""
    playwright = await async_playwright().start()
    browser = await playwright.chromium.launch(headless=True)
    page = await browser.new_page()
    await page.goto(file_path.as_uri())
    await asyncio.sleep(5)  # ç®€æ˜“ç­‰å¾…
    html_content = await page.content()
    await browser.close()
    await playwright.stop()
    return html_content

def save_beautiful_soup_content(beautiful_soup: List[Dict]) -> bool:
    """
    ä¿å­˜æå–çš„å†…å®¹åˆ°JSONæ–‡ä»¶
    """
    if beautiful_soup:
        out_path = THIS_DIR / "BeautifulSoup_Content.json"
        with open(out_path, "w", encoding="utf-8") as jf:
            json.dump(beautiful_soup, jf, ensure_ascii=False, indent=4)
        print(f"å·²å†™å…¥ JSONï¼š{out_path}")
        return True
    else:
        print("å…¬å…±çˆ¶èŠ‚ç‚¹ä¸‹æ²¡æœ‰æœ‰æ•ˆå­èŠ‚ç‚¹å†…å®¹ã€‚")
        return False

async def load_item_info(ctx, key: str = 'item') -> List[str]:
    """
    ä»item_info.jsonåŠ è½½ä¿¡æ¯
    keyå¯ä»¥æ˜¯'item'(å•†å“åç§°)æˆ–'price'(ä»·æ ¼)
    """
    product_names = []
    item_info_path = THIS_DIR / 'item_info.json'
    
    if not item_info_path.exists():
        await ctx.error(f"æ‰¾ä¸åˆ°item_info.jsonæ–‡ä»¶")
        return []
    
    try:
        with open(item_info_path, 'r', encoding='utf-8') as f:
            try:
                item_data = json.load(f)
                product_names = [str(item.get(key, '')) for item in item_data if key in item]
                await ctx.info(f"æ‰¾åˆ°{len(product_names)}ä¸ª{key}ä¿¡æ¯")
                return product_names
            except json.JSONDecodeError as e:
                await ctx.error(f"è§£æ{item_info_path}å¤±è´¥: {str(e)}")
                return []
    except Exception as e:
        await ctx.error(f"è¯»å–{item_info_path}å¤±è´¥: {str(e)}")
        return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æµç¨‹å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def process_tag_location(ctx, product_names: List[str], file_path: str | None = None, data_field: str = 'item') -> bool:
    """
    é€šç”¨çš„æ ‡ç­¾å®šä½å¤„ç†æµç¨‹(å¼‚æ­¥ç‰ˆæœ¬)
    
    Args:
        ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
        product_names: äº§å“åç§°åˆ—è¡¨ï¼ˆå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä»JSONåŠ è½½ï¼‰
        file_path: MHTMLæ–‡ä»¶è·¯å¾„
        data_field: è¦ä»JSONåŠ è½½çš„å­—æ®µåï¼Œå¯ä»¥æ˜¯ 'item', 'price' ç­‰
    """
    try:
        # 1. è·å–MHTMLæ–‡ä»¶
        await ctx.info(f"å¼€å§‹å¤„ç†æ ‡ç­¾å®šä½...")
        await ctx.info(f"å·¥ä½œç›®å½•: {Path.cwd()}")
        await ctx.info(f"MHTMLç›®å½•: {MHTML_DIR}")
        
        fp = await get_mhtml_file(file_path)
        await ctx.info(f"å¤„ç†MHTMLæ–‡ä»¶: {fp}")
        
        # 2. ç”¨PlaywrightåŠ è½½é¡µé¢è·å–HTMLå†…å®¹
        await ctx.info("å¼€å§‹ä½¿ç”¨PlaywrightåŠ è½½é¡µé¢...")
        html_content = await get_html_content(fp)
        soup = BeautifulSoup(html_content, "html.parser")
        await ctx.info(f"æˆåŠŸè·å– HTML å†…å®¹ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
        
        # 3. å¦‚æœæ²¡æœ‰æä¾›product_namesï¼Œåˆ™ä»item_info.jsonåŠ è½½æŒ‡å®šå­—æ®µ
        all_items = product_names if product_names else []
        
        if not all_items:
            item_info_path = THIS_DIR / 'item_info.json'
            if item_info_path.exists():
                with open(item_info_path, 'r', encoding='utf-8') as f:
                    try:
                        item_data = json.load(f)
                        # ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ä¼ å…¥çš„data_fieldå‚æ•°
                        all_items = [str(item.get(data_field, '')) for item in item_data if data_field in item and item.get(data_field)]
                        await ctx.info(f"ä»item_info.jsonåŠ è½½äº†{len(all_items)}ä¸ª{data_field}å­—æ®µçš„æ•°æ®")
                    except json.JSONDecodeError:
                        await ctx.warning("è§£æitem_info.jsonå¤±è´¥")
            else:
                await ctx.error(f"æœªæä¾›æ•°æ®ä¸”æ‰¾ä¸åˆ°item_info.jsonæ–‡ä»¶")
                return False
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„item
        if len(all_items) < 2:
            await ctx.warning(f"åªæœ‰{len(all_items)}ä¸ªæœ‰æ•ˆçš„{data_field}æ•°æ®ï¼Œè‡³å°‘éœ€è¦2ä¸ª")
            if not all_items:
                return False
        
        await ctx.info(f"å°†å¤„ç†{len(all_items)}ä¸ª{data_field}æ•°æ®: {all_items[:5]}{'...' if len(all_items) > 5 else ''}")
        
        # ================== æ–°å¢ï¼šå…ˆå°è¯•ç²¾ç¡®åŒ¹é… ==================
        await ctx.info("\nç¬¬ä¸€æ­¥ï¼šå°è¯•ç²¾ç¡®åŒ¹é…...")
        exact_match_tags = []
        
        # å¯¹æ¯ä¸ªitemå°è¯•ç²¾ç¡®åŒ¹é…
        for item in all_items:
            # 1. é¦–å…ˆå°è¯• string ç²¾ç¡®åŒ¹é… (æœ€ä¸¥æ ¼ï¼Œè¦æ±‚åªæœ‰ä¸€ä¸ªæ–‡æœ¬èŠ‚ç‚¹)
            exact_tag = soup.find(lambda t: t.string and t.string.strip().lower() == item.lower())
            if exact_tag:
                exact_match_tags.append(exact_tag)
                await ctx.info(f"  âœ“ stringç²¾ç¡®åŒ¹é…æˆåŠŸ: '{item}' -> <{exact_tag.name}>")
                continue
                
            # 2. ç„¶åå°è¯• get_text() ç²¾ç¡®åŒ¹é… (å¤„ç†æœ‰å­å…ƒç´ çš„æ ‡ç­¾)
            exact_tag = soup.find(lambda t: t.get_text().strip().lower() == item.lower())
            if exact_tag:
                exact_match_tags.append(exact_tag)
                await ctx.info(f"  âœ“ get_textç²¾ç¡®åŒ¹é…æˆåŠŸ: '{item}' -> <{exact_tag.name}>")
                continue
            
            # 3. æœ€åå°è¯•éƒ¨åˆ†åŒ¹é… (æœ€å®½æ¾)
            partial_tag = soup.find(lambda t: t.get_text() and item.lower() in t.get_text().strip().lower())
            if partial_tag:
                exact_match_tags.append(partial_tag)
                await ctx.info(f"  âœ“ éƒ¨åˆ†åŒ¹é…æˆåŠŸ: '{item}' åœ¨ <{partial_tag.name}> ä¸­")
        
        # å¦‚æœç²¾ç¡®åŒ¹é…æ‰¾åˆ°äº†è¶³å¤Ÿçš„æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨
        if len(exact_match_tags) >= 2:
            await ctx.info(f"\nç²¾ç¡®åŒ¹é…æˆåŠŸæ‰¾åˆ° {len(exact_match_tags)} ä¸ªæ ‡ç­¾ï¼Œè·³è¿‡åˆ†è¯æ­¥éª¤")
            
            # åªéšæœºé€‰æ‹©æœ€å¤šä¸¤ä¸ªæ ‡ç­¾å‚ä¸æœ€å°å…¬å…±çˆ¶å…ƒç´ æŸ¥æ‰¾
            selected_tags = exact_match_tags
            if len(exact_match_tags) > 2:
                selected_tags = random.sample(exact_match_tags, 2)
                await ctx.info(f"ä» {len(exact_match_tags)} ä¸ªç²¾ç¡®åŒ¹é…æ ‡ç­¾ä¸­éšæœºé€‰æ‹© 2 ä¸ªç”¨äºæŸ¥æ‰¾å…¬å…±çˆ¶å…ƒç´ ")
            
            # ä¿ç•™è¾“å…¥æ ‡ç­¾ä¿¡æ¯çš„è°ƒè¯•è¾“å‡º
            await ctx.info(f"\n===== å¼€å§‹æŸ¥æ‰¾ {len(selected_tags)} ä¸ªæ ‡ç­¾çš„æœ€ä½å…¬å…±çˆ¶å…ƒç´  =====")
            for i, tag in enumerate(selected_tags, 1):
                await ctx.info(f"è¾“å…¥æ ‡ç­¾ {i}: <{tag.name}> - æ–‡æœ¬: {tag.get_text().strip()[:50]}")
                await ctx.info(f"  DOMè·¯å¾„: {get_dom_path(tag)}")
                
            # å¯»æ‰¾è¿™äº›æ ‡ç­¾çš„æœ€å°å…¬å…±çˆ¶å…ƒç´ 
            common_parent = _lowest_common_parent(selected_tags)
            
            if not common_parent:
                await ctx.warning("æœªæ‰¾åˆ°åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„å…¬å…±çˆ¶å…ƒç´ ï¼Œå°è¯•ä½¿ç”¨åˆ†è¯æ–¹æ³•")
            elif common_parent.name in ['head', 'body', 'html']:
                await ctx.warning(f"ç²¾ç¡®åŒ¹é…çš„å…¬å…±çˆ¶å…ƒç´ æ˜¯ <{common_parent.name}>ï¼Œç»“æœå¤ªå¤§ï¼Œå°è¯•ä½¿ç”¨åˆ†è¯æ–¹æ³•")
            else:
                await ctx.info(f"ç²¾ç¡®åŒ¹é…æˆåŠŸæ‰¾åˆ°åˆé€‚çš„å…¬å…±çˆ¶å…ƒç´ : <{common_parent.name}>")
                # è·³è½¬åˆ°å¤„ç†å…¬å…±çˆ¶å…ƒç´ éƒ¨åˆ†ï¼Œä¸å†æ‰§è¡Œåˆ†è¯
                goto_process_common_parent = True
        else:
            await ctx.info(f"ç²¾ç¡®åŒ¹é…åªæ‰¾åˆ° {len(exact_match_tags)} ä¸ªæ ‡ç­¾ï¼Œä¸å¤Ÿç”¨ï¼Œç»§ç»­å°è¯•åˆ†è¯åŒ¹é…")
            goto_process_common_parent = False
        
        # ================== å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå†å°è¯•åˆ†è¯åŒ¹é… ==================
        if not goto_process_common_parent:
            await ctx.info("\nç¬¬äºŒæ­¥ï¼šå°è¯•åˆ†è¯åŒ¹é…...")
            # 4. å¯¹æ‰€æœ‰itemè¿›è¡Œåˆ†è¯
            tokenized_items = [_tokenize(item) for item in all_items]
            
            # æ‰¾å‡ºæœ€çŸ­çš„åˆ†è¯é•¿åº¦ï¼Œç¡®ä¿ä½ç½®å¯¹åº”
            min_token_length = min(len(tokens) for tokens in tokenized_items)
            await ctx.info(f"æœ€çŸ­åˆ†è¯é•¿åº¦: {min_token_length}")
            
            # 5. æŒ‰ä½ç½®å°è¯•åŒ¹é…ï¼Œå¯»æ‰¾DOMè·¯å¾„ç›¸ä¼¼çš„æ ‡ç­¾
            best_position = None
            best_position_tags = []
            best_similarity_score = 0.0
            
            for pos in range(min_token_length):
                # è·å–å½“å‰ä½ç½®çš„æ‰€æœ‰åˆ†è¯
                current_tokens = [item_tokens[pos] for item_tokens in tokenized_items]
                await ctx.info(f"\nå°è¯•ä½ç½®{pos+1}çš„åˆ†è¯: {', '.join(current_tokens[:5])}{'...' if len(current_tokens) > 5 else ''}")
                
                # æŸ¥æ‰¾åŒ¹é…æ ‡ç­¾
                position_tags = []
                for i, token in enumerate(current_tokens):
                    tag = soup.find(lambda t: t.string and token.lower() in t.string.lower())
                    if tag:
                        position_tags.append(tag)
                
                await ctx.info(f"  ä½ç½®{pos+1}æ‰¾åˆ°{len(position_tags)}/{len(current_tokens)}ä¸ªåŒ¹é…æ ‡ç­¾")
                
                # åªæœ‰æ‰¾åˆ°è¶³å¤Ÿå¤šçš„æ ‡ç­¾æ‰è¿›è¡ŒDOMè·¯å¾„æ¯”è¾ƒ
                if len(position_tags) >= 2:
                    # è®¡ç®—DOMè·¯å¾„çš„ç›¸ä¼¼åº¦
                    dom_paths = [get_dom_path(tag) for tag in position_tags]
                    
                    # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
                    path_similarities = []
                    for i in range(len(dom_paths)):
                        for j in range(i+1, len(dom_paths)):
                            similarity = _similar_ratio(dom_paths[i], dom_paths[j])
                            path_similarities.append(similarity)
                    
                    avg_similarity = sum(path_similarities) / len(path_similarities) if path_similarities else 0
                    await ctx.info(f"  DOMè·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
                    
                    # è®°å½•æœ€ä½³ä½ç½®
                    if avg_similarity > best_similarity_score:
                        best_similarity_score = avg_similarity
                        best_position = pos
                        best_position_tags = position_tags
                        await ctx.info(f"  âœ“ æ›´æ–°æœ€ä½³ä½ç½®ä¸ºä½ç½®{pos+1}ï¼Œç›¸ä¼¼åº¦{avg_similarity:.4f}")
            
            # 6. å¦‚æœæ‰¾åˆ°äº†æœ€ä½³ä½ç½®ï¼Œä½¿ç”¨è¯¥ä½ç½®çš„æ ‡ç­¾
            if best_position is not None and best_similarity_score > 0.6:  # è®¾ç½®ä¸€ä¸ªç›¸ä¼¼åº¦é˜ˆå€¼
                await ctx.info(f"\nä½¿ç”¨æœ€ä½³ä½ç½®{best_position+1}çš„æ ‡ç­¾ï¼ŒDOMè·¯å¾„ç›¸ä¼¼åº¦: {best_similarity_score:.4f}")
                # æ˜¾ç¤ºæ‰¾åˆ°çš„æ ‡ç­¾
                for i, tag in enumerate(best_position_tags[:5]):
                    await ctx.info(f"  æ ‡ç­¾{i+1}: <{tag.name}> - {tag.get_text().strip()[:50]}")
                    await ctx.info(f"  DOMè·¯å¾„: {get_dom_path(tag)}")
                
                # åªéšæœºé€‰æ‹©æœ€å¤šä¸¤ä¸ªæ ‡ç­¾å‚ä¸æœ€å°å…¬å…±çˆ¶å…ƒç´ æŸ¥æ‰¾
                selected_tags = best_position_tags
                if len(best_position_tags) > 2:
                    selected_tags = random.sample(best_position_tags, 2)
                    await ctx.info(f"\nä» {len(best_position_tags)} ä¸ªæ ‡ç­¾ä¸­éšæœºé€‰æ‹© 2 ä¸ªç”¨äºæŸ¥æ‰¾å…¬å…±çˆ¶å…ƒç´ ")
                
                # ä¿ç•™è¾“å…¥æ ‡ç­¾ä¿¡æ¯çš„è°ƒè¯•è¾“å‡º
                await ctx.info(f"\n===== å¼€å§‹æŸ¥æ‰¾ {len(selected_tags)} ä¸ªæ ‡ç­¾çš„æœ€ä½å…¬å…±çˆ¶å…ƒç´  =====")
                for i, tag in enumerate(selected_tags, 1):
                    await ctx.info(f"è¾“å…¥æ ‡ç­¾ {i}: <{tag.name}> - æ–‡æœ¬: {tag.get_text().strip()[:50]}")
                    await ctx.info(f"  DOMè·¯å¾„: {get_dom_path(tag)}")
                    
                # å¯»æ‰¾è¿™äº›æ ‡ç­¾çš„æœ€å°å…¬å…±çˆ¶å…ƒç´ 
                common_parent = _lowest_common_parent(selected_tags)
                
                if not common_parent:
                    await ctx.warning("æœªæ‰¾åˆ°åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„å…¬å…±çˆ¶å…ƒç´ ")
                    return False
                
                await ctx.info(f"æ‰¾åˆ°æœ€å°å…¬å…±çˆ¶å…ƒç´ : <{common_parent.name}>")
                
                # å¦‚æœå…¬å…±çˆ¶å…ƒç´ æ˜¯headæˆ–bodyï¼Œç›´æ¥æŠ¥é”™è€Œä¸æ˜¯ä½¿ç”¨å¤‡é€‰
                if common_parent.name in ['head', 'body', 'html']:
                    await ctx.error(f"å…¬å…±çˆ¶å…ƒç´ æ˜¯ <{common_parent.name}>ï¼ŒåŒ¹é…ç»“æœå¤ªå¤§ï¼Œå®šä½å¤±è´¥")
                    return False
            else:
                await ctx.warning("æœªæ‰¾åˆ°DOMè·¯å¾„ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜çš„ä½ç½®")
                # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
                await ctx.info("é€€å›åˆ°ä¼ ç»Ÿæ–¹æ³•ï¼Œä½¿ç”¨æ‰€æœ‰åˆ†è¯è¿›è¡ŒåŒ¹é…")
                
                # ä½¿ç”¨get_item_pathså‡½æ•°å¤„ç†å‰å‡ ä¸ªå•†å“åç§°
                sample_names = all_items[:3] if len(all_items) >= 3 else all_items
                paths = get_item_paths(soup, sample_names)
                await ctx.info(f"æ‰¾åˆ°çš„åŒ¹é…é¡¹æ•°é‡: {len(paths)}")
                
                # ç­›é€‰å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ ‡ç­¾
                majority_tags = filter_paths(paths)
                if not majority_tags:
                    await ctx.warning("æ²¡æœ‰åŒ¹é…åˆ°æœ‰æ•ˆçš„æ ‡ç­¾ï¼Œè·³è¿‡åç»­å¤„ç†ã€‚")
                    return False
                
                await ctx.info(f"é€‰å–äº†{len(majority_tags)}ä¸ªæœ€ä½³åŒ¹é…æ ‡ç­¾")
                
                # åªéšæœºé€‰æ‹©æœ€å¤šä¸¤ä¸ªæ ‡ç­¾å‚ä¸æœ€å°å…¬å…±çˆ¶å…ƒç´ æŸ¥æ‰¾
                selected_tags = majority_tags
                if len(majority_tags) > 2:
                    selected_tags = random.sample(majority_tags, 2)
                    await ctx.info(f"ä» {len(majority_tags)} ä¸ªæ ‡ç­¾ä¸­éšæœºé€‰æ‹© 2 ä¸ªç”¨äºæŸ¥æ‰¾å…¬å…±çˆ¶å…ƒç´ ")
                
                # æ‰¾æœ€ä½å…¬å…±çˆ¶èŠ‚ç‚¹
                common_parent = find_parent_with_multiple_descriptions(selected_tags)
                if not common_parent:
                    await ctx.warning("æœªæ‰¾åˆ°åŒ…å«æ‰€æœ‰æè¿°çš„å…¬å…±çˆ¶å…ƒç´ ã€‚")
                    return False

                # å¦‚æœå…¬å…±çˆ¶å…ƒç´ æ˜¯headæˆ–bodyï¼Œç›´æ¥æŠ¥é”™
                if common_parent.name in ['head', 'body', 'html']:
                    await ctx.error(f"å…¬å…±çˆ¶å…ƒç´ æ˜¯ <{common_parent.name}>ï¼ŒåŒ¹é…ç»“æœå¤ªå¤§ï¼Œå®šä½å¤±è´¥")
                    return False
        
        # ================== å¤„ç†æ‰¾åˆ°çš„å…¬å…±çˆ¶å…ƒç´  ==================
        await ctx.info(f"\næœ€ç»ˆä½¿ç”¨çš„çˆ¶å…ƒç´ : <{common_parent.name}> - å†…å®¹é¢„è§ˆ: {common_parent.get_text().strip()[:100]}...")
        
        # 7. éå†å…¬å…±çˆ¶èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ï¼Œæå–å†…å®¹å¹¶å†™å…¥JSON
        beautiful_soup = []
        child_count = 0
        for idx, child in enumerate(common_parent.children, start=1):
            child_count += 1
            if getattr(child, "prettify", None):
                content = child.prettify().strip()
                if content:
                    beautiful_soup.append({
                        "Order": idx,
                        "Content": content
                    })
        
        await ctx.info(f"å…¬å…±çˆ¶å…ƒç´ æ€»å…±æœ‰ {child_count} ä¸ªå­å…ƒç´ ")
        await ctx.info(f"æå–å‡º {len(beautiful_soup)} ä¸ªæœ‰æ•ˆå­å…ƒç´ ")
        
        if beautiful_soup:
            out_path = THIS_DIR / "BeautifulSoup_Content.json"
            with open(out_path, "w", encoding="utf-8") as jf:
                json.dump(beautiful_soup, jf, ensure_ascii=False, indent=4)
            await ctx.info(f"å·²å†™å…¥JSON: {out_path}")
            return True
        else:
            await ctx.warning("å…¬å…±çˆ¶èŠ‚ç‚¹ä¸‹æ²¡æœ‰æœ‰æ•ˆå­èŠ‚ç‚¹å†…å®¹ã€‚")
            await ctx.warning("å¯èƒ½çš„åŸå› ï¼š")
            await ctx.warning("1. å…¬å…±çˆ¶å…ƒç´ ä¸ºç©ºæˆ–åªåŒ…å«æ–‡æœ¬èŠ‚ç‚¹")
            await ctx.warning("2. å­å…ƒç´ æ— æ³•prettifyï¼ˆå¯èƒ½æ˜¯æ–‡æœ¬èŠ‚ç‚¹ï¼‰")
            return False
    
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        await ctx.error(f"å¤„ç†æ ‡ç­¾å®šä½æ—¶å‡ºé”™: {str(e)}")
        await ctx.error(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
        return False

async def process_name_tag_location(ctx, file_path: str | None = None) -> bool:
    """
    å¤„ç†å•†å“åç§°æ ‡ç­¾å®šä½
    """
    await ctx.info("å¼€å§‹å¤„ç†å•†å“åç§°æ ‡ç­¾å®šä½...")
    
    # ä»item_info.jsonåŠ è½½å•†å“åç§°
    try:
        # å°è¯•å…ˆåŠ è½½itemå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µ
        product_names = await load_item_info(ctx, key='item')
        
        if not product_names:
            # å¦‚æœæ²¡æœ‰itemå­—æ®µï¼Œå°è¯•ä½¿ç”¨priceå­—æ®µä½œä¸ºæ›¿ä»£
            await ctx.warning("æœªæ‰¾åˆ°itemå­—æ®µï¼Œå°è¯•ä½¿ç”¨priceå­—æ®µ")
            product_names = await load_item_info(ctx, key='price')
            
            if not product_names:
                await ctx.error("æœªæ‰¾åˆ°itemæˆ–priceå­—æ®µä¿¡æ¯")
                return False
        
        # æ‰§è¡Œæ ‡ç­¾å®šä½å¤„ç†ï¼Œä¼ å…¥æ•°æ®ç±»å‹
        field_type = 'item' if 'item' in str(product_names[0]) else 'price'
        result = await process_tag_location(ctx, product_names, file_path, data_field=field_type)
        if result:
            await ctx.info("å•†å“åç§°æ ‡ç­¾å®šä½å¤„ç†å®Œæˆ")
        else:
            await ctx.warning("å•†å“åç§°æ ‡ç­¾å®šä½å¤„ç†å¤±è´¥")
        return result
                
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        await ctx.error(f"å•†å“åç§°æ ‡ç­¾å®šä½å¤„ç†å‡ºé”™: {str(e)}")
        await ctx.error(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
        return False

async def process_price_tag_location(ctx, file_path: str | None = None) -> bool:
    """
    å¤„ç†å•†å“ä»·æ ¼æ ‡ç­¾å®šä½
    """
    await ctx.info("å¼€å§‹å¤„ç†å•†å“ä»·æ ¼æ ‡ç­¾å®šä½...")
    
    # ä»item_info.jsonåŠ è½½ä»·æ ¼ä¿¡æ¯
    try:
        price_info = await load_item_info(ctx, key='price')
        
        if not price_info:
            await ctx.error("æœªæ‰¾åˆ°priceå­—æ®µä¿¡æ¯")
            return False
        
        # æ‰§è¡Œæ ‡ç­¾å®šä½å¤„ç†
        result = await process_tag_location(ctx, price_info, file_path, data_field='price')
        if result:
            await ctx.info("å•†å“ä»·æ ¼æ ‡ç­¾å®šä½å¤„ç†å®Œæˆ")
        else:
            await ctx.warning("å•†å“ä»·æ ¼æ ‡ç­¾å®šä½å¤„ç†å¤±è´¥")
        return result
                
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        await ctx.error(f"å•†å“ä»·æ ¼æ ‡ç­¾å®šä½å¤„ç†å‡ºé”™: {str(e)}")
        await ctx.error(f"é”™è¯¯è¯¦æƒ…: {error_trace}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CliContext:
    """å‘½ä»¤è¡Œå·¥å…·çš„ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œæ¨¡æ‹ŸMCP Contextæ¥å£"""
    async def info(self, msg):
        print(f"[INFO] {msg}")
    
    async def warning(self, msg):
        print(f"[WARNING] {msg}")
    
    async def error(self, msg):
        print(f"[ERROR] {msg}")

async def main_async():
    parser = argparse.ArgumentParser(description="Tag locating tool")
    parser.add_argument("--type", choices=["name", "price"], default="name", 
                        help="Processing type: name (product name) or price")
    parser.add_argument("--filepath", default=None,
                        help="Path to the MHTML file to process (optional, defaults to the latest file in mhtml_output)")
    args = parser.parse_args()
    
    ctx = CliContext()
    
    if args.type == "name":
        await process_name_tag_location(ctx, args.filepath)
    else:
        await process_price_tag_location(ctx, args.filepath)

if __name__ == "__main__":
    asyncio.run(main_async())
